# 2026-01-15-task2
#datawhale学习笔记 



## 一、NLP的主要任务（1.3）——计算机处理语言的"工种"

简而言之，NLP（Natural Language Processing）就是让计算机能"理解"人话。但这件事太大了，所以被拆成了很多小任务，每个任务来解决一个具体问题。

### 1. 中文分词

英文有空格天然分开单词，但中文"我爱自然语言处理"是连在一起的。计算机不知道该切成"我/爱/自然语言处理"还是"我/爱/自然/语言/处理"。这一步做不好，后面全乱套。所以中文分词是中文NLP的第一道关卡。

### 2. 词性标注

分完词之后，还要告诉计算机每个词是什么"身份"：名词、动词、形容词……比如"苹果好吃"里，"苹果"是名词，"好吃"是形容词。常用的模型包括HMM（隐马尔可夫）、CRF（条件随机场），以及现在更流行的深度学习方法。

### 3. 命名实体识别（NER）

从文本里识别出人名、地名、机构名等特殊实体。比如"张三去了北京"，模型要能认出"张三"是人名、"北京"是地名。这对于构建知识图谱、智能问答非常重要。

### 4. 文本分类

给定一段文本，判断它属于哪个类别。垃圾邮件检测、新闻分类（体育/政治/科技）、情感分析（判断评论是好评还是差评）都属于这个任务。关键在于：选好特征 + 选对算法 + 有高质量的训练数据。

### 5. 其他进阶任务

还有**关系抽取**（找出实体之间的关系）、**文本摘要**（把长文章压缩成短摘要）、**机器翻译**、**自动问答**等。越往后，任务越复杂，对模型的语义理解能力要求越高。

---

## 二、文本表示（1.4）——把文字变成数字

计算机只认数字，所以必须把文字"翻译"成数学形式（通常是向量）。这就是**文本表示**，它是所有NLP任务的地基。

### 1. 向量空间模型（VSM）

把每篇文档表示成一个高维向量，每个维度对应词典里的一个词，数值是该词的权重（比如用TF-IDF计算）。

**问题**：词典有几万个词，向量就有几万维，而且大部分位置是0（非常稀疏），既浪费存储空间，又丢失了词序和语义信息。

### 2. Word2Vec

2013年，Mikolov提出了Word2Vec。核心思想很直觉：**经常出现在相似上下文里的词，意思应该也相似**。

它用神经网络学习，把每个词压缩成几百维的"密集向量"。训练完之后，语义相近的词在向量空间里距离也近。最经典的例子："国王 - 男人 + 女人 ≈ 王后"，居然能做语义计算！

**局限性**：同一个词永远只有一个向量。但"苹果"在"吃苹果"和"苹果手机"里意思完全不同，Word2Vec处理不了这种**一词多义**问题。

### 3. ELMo / BERT

后来的ELMo、BERT等预训练模型解决了这个问题。它们能根据上下文**动态生成**词向量，同一个词在不同句子里会有不同的表示。这也正式开启了"预训练+微调"的大模型时代。

---

## 三、小结

| 章节          | 核心问题        | 理解                             |
| ----------- | ----------- | ------------------------------ |
| 1.3 NLP主要任务 | 计算机要"做什么"   | 分词、标注、分类、翻译……每个任务解决一个具体的语言处理问题 |
| 1.4 文本表示    | 计算机怎么"看懂"文字 | 把人话变成数字向量，是所有NLP任务的基础          |

**核心逻辑**：先用文本表示把语言转成数字，才能进行后续的分词、分类、翻译等操作。从稀疏的VSM到密集的Word2Vec，再到动态的BERT，文本表示技术的进步直接推动了整个NLP领域的发展。

